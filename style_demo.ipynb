{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image as Image2\n",
    "from collections import namedtuple\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from ipywidgets import interact, interactive\n",
    "from transformernet import TransformerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image with optional size or scale\n",
    "def load_image(filename, size=None, scale=None):\n",
    "    img = Image2.open(filename)\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size), Image2.ANTIALIAS)\n",
    "    elif scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image2.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "# save image\n",
    "def save_image(filename, data):\n",
    "    img = data.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
    "    img = Image2.fromarray(img)\n",
    "    img.save(filename)\n",
    "\n",
    "# return gram matrix of input layer outputs\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    # compute gram product\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std. normalize so we can easily put it in a nn.Sequential\n",
    "    # allows to work with image Tenor of shape\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    return (batch - mean) / std\n",
    "\n",
    "\n",
    "def stylize(style, content):\n",
    "    # check if we can use a gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    #stylize image\n",
    "    content_image = load_image(content + \".jpg\", size=None)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        style_model = TransformerNet()\n",
    "        # INPUT, model you want to stylize with\n",
    "        state_dict = torch.load(\"models/\" + style + \".model\")\n",
    "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(state_dict.keys()):\n",
    "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del state_dict[k]\n",
    "        style_model.load_state_dict(state_dict)\n",
    "        style_model.to(device)\n",
    "        output = style_model(content_image).cpu()\n",
    "    save_image(content + '_' + style + \".jpg\", output[0])\n",
    "    #i_style = Image(filename='styles/' + style + '.jpg')\n",
    "    #i_content = Image(filename=content + '.jpg')\n",
    "    i_result = Image(filename=content + '_' + style + \".jpg\")\n",
    "    #display(i_style)\n",
    "    #display(i_content)\n",
    "    display(i_result)\n",
    "def display_style(x):\n",
    "    i = Image(filename='styles/' + x + '.jpg')\n",
    "    display(i)\n",
    "    \n",
    "def display_content(x):\n",
    "    i = Image(filename=x + '.jpg')\n",
    "    display(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_suite = ['bullfight', '2500_bullfight', '5000_bullfight', '10000_bullfight', '20000_bullfight', '40000_bullfight', 'candy', 'mosaic','digbychildren','funeraryslab','mosaic','mounthood','plumblossoms','streetscene','tarzana','udnie']\n",
    "content_suite = ['law_library','reggie','harbaugh','centraldiag','umma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfb12848a124765a09216eecd3a0acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('bullfight', '2500_bullfight', '5000_bullfight', '100…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_style(x)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(display_style, x = style_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ff005da8a947c486c18849de57580c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('law_library', 'reggie', 'harbaugh', 'centraldiag', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_content(x)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(display_content, x = content_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ced44a6c430415e911709f2e57194fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='style', options=('bullfight', '2500_bullfight', '5000_bullfight', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.stylize(style, content)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(stylize, style = style_suite, content = content_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
